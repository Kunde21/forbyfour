// +build !noasm !appengine

#define NOSPLIT 7

// func initasm()(a,a2 bool)
// pulled from runtime/asm_amd64.s
TEXT ·initasm(SB), NOSPLIT, $0
	MOVQ $1, R15

	XORQ R9, R9
	MOVQ $1, AX
	CPUID
	MOVQ CX, R10
	ANDL $0x1, CX
	CMPL CX, $0x1
	CMOVQEQ R15, R9
	MOVB R9, ·sse3Supt(SB)

	XORQ R9,R9	
	MOVQ R10, CX
	ANDL $0x18000000, CX //  OSXSAVE and AVX bits
	CMPL CX, $0x18000000 // For XGETBV, OSXSAVE bit is required and sufficient
	JNE  noavx

	XORQ R9, R9
	ANDL $0x1000, R10
	CMPL R10, $0x1000
	CMOVQEQ R15, R9
	MOVB R9, ·fmaSupt(SB) // set numgo·fmaSupt

	
	MOVL $0, CX
	XGETBV
	ANDL $6, AX
	CMPL AX, $6		// Check for OS support of YMM registers
	JNE  noavx
	MOVB $1, ·avxSupt(SB)

	// Check for AVX2 capability
	XORQ R9,R9
	MOVL $7, AX
	MOVL $0, CX
	CPUID
	ANDL $0x20, BX         // check for AVX2 bit
	CMPL BX, $0x20

	CMOVQEQ R15, R9
	MOVB 	R9, ·avx2Supt(SB)
	RET
noavx:	// OS or Hardware doesn't support VEX enc
	MOVB $0, ·fmaSupt(SB)
	MOVB $0, ·avxSupt(SB) 
	MOVB $0, ·avx2Supt(SB)
	RET


// func innrPrd_avx(a,b,c []float64)
TEXT ·innrPrd_avx(SB), NOSPLIT, $0
	MOVQ a_base+0(FP), R8
	MOVQ b_base+24(FP), R9
	MOVQ c_base+48(FP), R10

	// Load Column data
	VMOVDQA (R9), Y0
	VMOVDQA 32(R9), Y1
	
	// Unpack columns, matching high and low 128-bit pairs
	BYTE $0xC5 ; BYTE $0xFD ; BYTE $0x14 ; BYTE $0xE1
	//VUNPCKLPD Y0, Y1, Y4
	BYTE $0xC5 ; BYTE $0xFD ; BYTE $0x15 ; BYTE $0xF1
	//VUNPCKHPD Y0, Y1, Y6

	VMOVDQA 64(R9), Y2
	VMOVDQA 96(R9), Y3
	
	BYTE $0xC5 ; BYTE $0xED ; BYTE $0x14 ; BYTE $0xEB
	//VUNPCKLPD Y2, Y3, Y5
	BYTE $0xC5 ; BYTE $0xED ; BYTE $0x15 ; BYTE $0xFB
	//VUNPCKHPD Y2, Y3, Y7

	// Match the pairs created in unpacking
	BYTE $0xC4 ; BYTE $0xE3 ; BYTE $0x5D ; BYTE $0x06 ; BYTE $0xC5 ; BYTE $0x20
	//VPERM2F128 Y4, Y5, Y0 $0x20
	BYTE $0xC4 ; BYTE $0xE3 ; BYTE $0x4D ; BYTE $0x06 ; BYTE $0xCF ; BYTE $0x20
	//VPERM2F128 Y6, Y7, Y1 $0x20
	BYTE $0xC4 ; BYTE $0xE3 ; BYTE $0x5D ; BYTE $0x06 ; BYTE $0xD5 ; BYTE $0x31
	//VPERM2F128 Y4, Y5, Y2 $0x31
	BYTE $0xC4 ; BYTE $0xE3 ; BYTE $0x4D ; BYTE $0x06 ; BYTE $0xDF ; BYTE $0x31
	//VPERM2F2128 Y6, Y7, Y3, $0x31

	// Column data is now organized in ymm0-3
	// Load single4 Row data
	VMOVDQA (R8), Y4

	BYTE $0xC5 ; BYTE $0xDD ; BYTE $0x59 ; BYTE $0xE8
	//VMULPD Y4, Y0, Y5
	BYTE $0xC5 ; BYTE $0xDD ; BYTE $0x59 ; BYTE $0xF1
	//VMULPD Y4, Y1, Y6

	PREFETCHT0 (R10)
	
	BYTE $0xC5 ; BYTE $0xDD ; BYTE $0x59 ; BYTE $0xFA
	//VMULPD Y4, Y2, Y7
	BYTE $0xC5 ; BYTE $0x5D ; BYTE $0x59 ; BYTE $0xC3
	//VMULPD Y4, Y3, Y8

	VMOVDQA 32(R8), Y9
	
	BYTE $0xC5 ; BYTE $0x35 ; BYTE $0x59 ; BYTE $0xD0
	//VMULPD Y9, Y0, Y10
	BYTE $0xC5 ; BYTE $0x35 ; BYTE $0x59 ; BYTE $0xD9
	//VMULPD Y9, Y1, Y11

	PREFETCHT0 32(R10)
	
	BYTE $0xC5 ; BYTE $0x35 ; BYTE $0x59 ; BYTE $0xE2
	//VMULPD Y9, Y2, Y12
	BYTE $0xC5 ; BYTE $0x35 ; BYTE $0x59 ; BYTE $0xEB
	//VMULPD Y9, Y3, Y13

	VMOVDQA 64(R8), Y14

	BYTE $0xC5; BYTE $0xD5; BYTE $0x7C; BYTE $0xEE	
	//VHADDPD Y5, Y6, Y5
	BYTE $0xC4; BYTE $0xC1; BYTE $0x45; BYTE $0x7C; BYTE $0xF0
	//VHADDPD Y7, Y8, Y6

	BYTE $0xC4; BYTE $0x41; BYTE $0x2D; BYTE $0x7C; BYTE $0xD3
	//VHADDPD Y10, Y11, Y10

	BYTE $0xC4; BYTE $0x41; BYTE $0x1D; BYTE $0x7C; BYTE $0xDD
	//VHADDPD Y12, Y13, Y11

	VMOVDQA 96(R8), Y15
	
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x06; BYTE $0xFE; BYTE $0x31
	//VPERM2F128 Y5, Y6, Y7, $0x31	
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x06; BYTE $0xF6; BYTE $0x20
	//VPERM2F128 Y5, Y6, Y6, $0x20

	BYTE $0xC5; BYTE $0xCD; BYTE $0x58; BYTE $0xE7
	//VADDPD Y6, Y7, Y4
	VMOVDQA Y4, (R10)

	BYTE $0xC4; BYTE $0x43; BYTE $0x2D; BYTE $0x06; BYTE $0xE3; BYTE $0x20
	//VPERM2F128 Y10, Y11, Y12, $0x20
	BYTE $0xC4; BYTE $0x43; BYTE $0x2D; BYTE $0x06; BYTE $0xEB; BYTE $0x31
	//VERPM2F128 Y10, Y11, Y13, $0X31
	
	BYTE $0xC4; BYTE $0x41; BYTE $0x1D; BYTE $0x58; BYTE $0xCD
	//VADDPD Y12, Y13, Y9
	VMOVDQA Y9, 32(R10)
	
	BYTE $0xC5 ; BYTE $0x0D ; BYTE $0x59 ; BYTE $0xD0
	//VMULPD Y14, Y0, Y10 vmulpd ymm10,ymm14,ymm0
	BYTE $0xC5 ; BYTE $0x0D ; BYTE $0x59 ; BYTE $0xD9
	//VMULPD Y14, Y1, Y11
	BYTE $0xC5 ; BYTE $0x0D ; BYTE $0x59 ; BYTE $0xE2
	//VMULPD Y14, Y2, Y12
	BYTE $0xC5 ; BYTE $0x0D ; BYTE $0x59 ; BYTE $0xEB
	//VMULPD Y14, Y3, Y13

	PREFETCHT0 64(R10)

	BYTE $0xC5 ; BYTE $0x85 ; BYTE $0x59 ; BYTE $0xE8
	//VMULPD Y15, Y0, Y5
	BYTE $0xC5 ; BYTE $0x85 ; BYTE $0x59 ; BYTE $0xF1
	//VMULPD Y15, Y1, Y6

	PREFETCHT0 96(R10)
	
	BYTE $0xC5 ; BYTE $0x85 ; BYTE $0x59 ; BYTE $0xFA
	//VMULPD Y15, Y2, Y7
	BYTE $0xC5 ; BYTE $0x05 ; BYTE $0x59 ; BYTE $0xC3
	//VMULPD Y15, Y3, Y8
	
	BYTE $0xC5; BYTE $0xD5; BYTE $0x7C; BYTE $0xEE	
	//VHADDPD Y5, Y6, Y5
	BYTE $0xC4; BYTE $0xC1; BYTE $0x45; BYTE $0x7C; BYTE $0xF0
	//VHADDPD Y7, Y8, Y6

	BYTE $0xC4; BYTE $0x41; BYTE $0x2D; BYTE $0x7C; BYTE $0xD3
	//VHADDPD Y10, Y11, Y10
	BYTE $0xC4; BYTE $0x41; BYTE $0x1D; BYTE $0x7C; BYTE $0xDD
	//VHADDPD Y12, Y13, Y11

	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x06; BYTE $0xFE; BYTE $0x31
	//VPERM2F128 Y5, Y6, Y7, $0x31	
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x06; BYTE $0xF6; BYTE $0x20
	//VPERM2F128 Y5, Y6, Y6, $0x20

	BYTE $0xC5; BYTE $0xCD; BYTE $0x58; BYTE $0xE7
	//VADDPD Y6, Y7, Y4
	VMOVDQA Y4, 96(R10)

	BYTE $0xC4; BYTE $0x43; BYTE $0x2D; BYTE $0x06; BYTE $0xE3; BYTE $0x20
	//VPERM2F128 Y10, Y11, Y12, $0x20
	BYTE $0xC4; BYTE $0x43; BYTE $0x2D; BYTE $0x06; BYTE $0xEB; BYTE $0x31
	//VERPM2F128 Y10, Y11, Y13, $0X31
	
	BYTE $0xC4; BYTE $0x41; BYTE $0x1D; BYTE $0x58; BYTE $0xCD
	//VADDPD Y12, Y13, Y9
	VMOVDQA Y9, 64(R10)

	//Cleanup
	VZEROUPPER
	
	RET

// func innrPrd_avx2(a,b,c []float64)
TEXT ·innrPrd_avx2(SB), NOSPLIT, $0
	MOVQ a_base+0(FP), R8
	MOVQ b_base+24(FP), R9
	MOVQ c_base+48(FP), R10
	MOVQ $0, DI  // Hard-coded for 4x4 matrix

	// Load Column data
	VMOVDQA (R9), Y0
	VMOVDQA 32(R9), Y1

	// Unpack columns, matching high and low 128-bit pairs
	BYTE $0xC5 ; BYTE $0xFD ; BYTE $0x14 ; BYTE $0xE1
	//VUNPCKLPD Y0, Y1, Y4
	BYTE $0xC5 ; BYTE $0xFD ; BYTE $0x15 ; BYTE $0xF1
	//VUNPCKHPD Y0, Y1, Y6

	VMOVDQA 64(R9), Y2
	VMOVDQA 96(R9), Y3
	
	BYTE $0xC5 ; BYTE $0xED ; BYTE $0x14 ; BYTE $0xEB
	//VUNPCKLPD Y2, Y3, Y5
	BYTE $0xC5 ; BYTE $0xED ; BYTE $0x15 ; BYTE $0xFB
	//VUNPCKHPD Y2, Y3, Y7

	// Match the pairs created in unpacking
	BYTE $0xC4 ; BYTE $0xE3 ; BYTE $0x5D ; BYTE $0x06 ; BYTE $0xC5 ; BYTE $0x20
	//VPERM2F128 Y4, Y5, Y0, $0x20
	BYTE $0xC4 ; BYTE $0xE3 ; BYTE $0x4D ; BYTE $0x06 ; BYTE $0xCF ; BYTE $0x20
	//VPERM2F128 Y6, Y7, Y1, $0x20
	BYTE $0xC4 ; BYTE $0xE3 ; BYTE $0x5D ; BYTE $0x06 ; BYTE $0xD5 ; BYTE $0x31
	//VPERM2F128 Y4, Y5, Y2, $0x31
	BYTE $0xC4 ; BYTE $0xE3 ; BYTE $0x4D ; BYTE $0x06 ; BYTE $0xDF ; BYTE $0x31
	//VPERM2F128 Y6, Y7, Y3, $0x31
	
	// Column data is now organized in ymm0-3
loop:	// Load single4 Row data
	VMOVDQA (R8)(DI*8), Y4

	BYTE $0xC5 ; BYTE $0xDD ; BYTE $0x59 ; BYTE $0xE8
	//VMULPD Y4, Y0, Y5
	BYTE $0xC5 ; BYTE $0xDD ; BYTE $0x59 ; BYTE $0xF1
	//VMULPD Y4, Y1, Y6
	BYTE $0xC5 ; BYTE $0xDD ; BYTE $0x59 ; BYTE $0xFA
	//VMULPD Y4, Y2, Y7
	BYTE $0xC5 ; BYTE $0x5D ; BYTE $0x59 ; BYTE $0xC3
	//VMULPD Y4, Y3, Y8
	
	BYTE $0xC5; BYTE $0xD5; BYTE $0x7C; BYTE $0xEE	
	//vhaddpd ymm5,ymm6,ymm5
	BYTE $0xC4; BYTE $0xC1; BYTE $0x45; BYTE $0x7C; BYTE $0xF0
	//vhaddpd ymm6,ymm8,ymm7
	
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x06; BYTE $0xFE; BYTE $0x31
	//vperm2f128 ymm7,ymm5,ymm6,0x31
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x06; BYTE $0xF6; BYTE $0x20
	//vperm2f128 ymm6,ymm5,ymm6,0x20

	
	BYTE $0xC5; BYTE $0xCD; BYTE $0x58; BYTE $0xE7
	//vaddpd ymm4,ymm6,ymm7
	
	VMOVDQA Y4, (R10)(DI*8)

	ADDQ $4, DI
	CMPQ DI, $16
	JL loop

	//Cleanup
	VZEROUPPER
	
	RET

// func innrPrd_fma(a,b,c []float64)
// Requires FMA, uses VFMADD213PD instruction
// Requires SSE3, uses HADDPD instruction
TEXT ·innrPrd_sse3_fma(SB), NOSPLIT, $0
	MOVQ a_base+0(FP), R8
	MOVQ b_base+24(FP), R9
	MOVQ c_base+48(FP), R10
	MOVQ $16, SI  // Hard-coded for 4x4 matrix

	MOVAPD (R9), X0
	MOVAPD 16(R9), X4
	MOVAPD 32(R9), X8
	MOVAPD 48(R9), X9

	MOVAPD X0, X2
	UNPCKLPD X8, X0
	UNPCKHPD X8, X2
	
	MOVAPD X4, X6
	UNPCKLPD X9, X4
	UNPCKHPD X9, X6

	MOVAPD 64(R9), X1
	MOVAPD 80(R9), X5
	MOVAPD 96(R9), X8
	MOVAPD 112(R9), X9
	
	MOVAPD X1, X3
	UNPCKLPD X8, X1
	UNPCKHPD X8, X3

	MOVAPD X5, X7
	UNPCKLPD X9, X5
	UNPCKHPD X9, X7
	// Column data is now organized in xmm0-7
	
loop:	// Load single Row data
	MOVAPD (R8), X8
	MOVAPD X8, X10
	MOVAPD 16(R8), X9
	MOVAPD X9, X11

	MULPD X0, X10
	BYTE $0xC4; BYTE $0x42; BYTE $0xF1; BYTE $0xA8; BYTE $0xDA
	//VFMADD213PD X1, X10, X11

	MOVAPD X8, X10
	MOVAPD X9, X12

	MULPD X2, X10
	BYTE $0xC4; BYTE $0x42; BYTE $0xE1; BYTE $0xA8; BYTE $0xE2
	//VFMADD213PD X3, X10, X12

	HADDPD X12, X11
	MOVAPD X11, (R10)

	MOVAPD X8, X10
	MOVAPD X9, X11

	MULPD X4, X10
	BYTE $0xC4; BYTE $0x42; BYTE $0xD1; BYTE $0xA8; BYTE $0xDA
	//VFMADD213PD X1, X10, X11

	MOVAPD X8, X10
	MOVAPD X9, X12

	MULPD X6, X10
	BYTE $0xC4; BYTE $0x42; BYTE $0xC1; BYTE $0xA8; BYTE $0xE2
	//VFMADD213PD X3, X10, X12

	HADDPD X12, X11
	MOVAPD X11, 16(R10)

	ADDQ $32, R8
	ADDQ $32, R10
	SUBQ $4, SI
	JG loop

end:	
	RET

// func innrPrd_fma(a,b,c []float64)
// Requires FMA, uses VFMADD213PD instruction
TEXT ·innrPrd_sse2_fma(SB), NOSPLIT, $0
	MOVQ a_base+0(FP), R8
	MOVQ b_base+24(FP), R9
	MOVQ c_base+48(FP), R10
	MOVQ $16, SI  // Hard-coded for 4x4 matrix

	MOVAPD (R9), X0
	MOVAPD 16(R9), X4
	MOVAPD 32(R9), X8
	MOVAPD 48(R9), X9

	MOVAPD X0, X2
	UNPCKLPD X8, X0
	UNPCKHPD X8, X2
	
	MOVAPD X4, X6
	UNPCKLPD X9, X4
	UNPCKHPD X9, X6

	MOVAPD 64(R9), X1
	MOVAPD 80(R9), X5
	MOVAPD 96(R9), X8
	MOVAPD 112(R9), X9
	
	MOVAPD X1, X3
	UNPCKLPD X8, X1
	UNPCKHPD X8, X3

	MOVAPD X5, X7
	UNPCKLPD X9, X5
	UNPCKHPD X9, X7
	// Column data is now organized in xmm0-7
	
loop:	// Load single Row data
	MOVAPD (R8), X8
	MOVAPD X8, X10
	MOVAPD 16(R8), X9
	MOVAPD X9, X11

	MULPD X0, X10
	BYTE $0xC4; BYTE $0x42; BYTE $0xF1; BYTE $0xA8; BYTE $0xDA
	//VFMADD213PD X1, X10, X11

	MOVAPD X8, X10
	MOVAPD X9, X12

	MULPD X2, X10
	BYTE $0xC4; BYTE $0x42; BYTE $0xE1; BYTE $0xA8; BYTE $0xE2
	//VFMADD213PD X3, X10, X12

	MOVAPD X11, X10
	UNPCKLPD X12, X11
	UNPCKHPD X12, X10
	ADDPD X10, X11 

	MOVAPD X11, (R10)

	MOVAPD X8, X10
	MOVAPD X9, X11

	MULPD X4, X10
	BYTE $0xC4; BYTE $0x42; BYTE $0xD1; BYTE $0xA8; BYTE $0xDA
	//VFMADD213PD X1, X10, X11

	MOVAPD X8, X10
	MOVAPD X9, X12

	MULPD X6, X10
	BYTE $0xC4; BYTE $0x42; BYTE $0xC1; BYTE $0xA8; BYTE $0xE2
	//VFMADD213PD X3, X10, X12

	MOVAPD X11, X10
	UNPCKLPD X12, X11
	UNPCKHPD X12, X10
	ADDPD X10, X11 

	MOVAPD X11, 16(R10)

	ADDQ $32, R8
	ADDQ $32, R10
	SUBQ $4, SI
	JG loop

end:	
	RET
	
// func innrPrd_sse3(a,b,c []float64)
// Requres SSE3 for HADDPD instruction
TEXT ·innrPrd_sse3(SB), NOSPLIT, $0
	MOVQ a_base+0(FP), R8
	MOVQ b_base+24(FP), R9
	MOVQ c_base+48(FP), R10
	MOVQ $16, SI  // Hard-coded for 4x4 matrix

	MOVAPD (R9), X0
	MOVAPD 16(R9), X4
	MOVAPD 32(R9), X8
	MOVAPD 48(R9), X9

	MOVAPD X0, X2
	UNPCKLPD X8, X0
	UNPCKHPD X8, X2
	
	MOVAPD X4, X6
	UNPCKLPD X9, X4
	UNPCKHPD X9, X6

	MOVAPD 64(R9), X1
	MOVAPD 80(R9), X5
	MOVAPD 96(R9), X8
	MOVAPD 112(R9), X9
	
	MOVAPD X1, X3
	UNPCKLPD X8, X1
	UNPCKHPD X8, X3

	MOVAPD X5, X7
	UNPCKLPD X9, X5
	UNPCKHPD X9, X7
	// Column data is now organized in xmm0-7
	
loop:	// Load single Row data
	MOVAPD (R8), X8
	MOVAPD X8, X10
	MOVAPD 16(R8), X9
	MOVAPD X9, X11

	MULPD X0, X10
	MULPD X1, X11
	ADDPD X10, X11

	MOVAPD X8, X10
	MOVAPD X9, X12

	MULPD X2, X10
	MULPD X3, X12
	ADDPD X10, X12

	HADDPD X12, X11
	MOVAPD X11, (R10)

	MOVAPD X8, X10
	MOVAPD X9, X11

	MULPD X4, X10
	MULPD X5, X11
	ADDPD X10, X11

	MOVAPD X8, X10
	MOVAPD X9, X12

	MULPD X6, X10
	MULPD X7, X12
	ADDPD X10, X12

	HADDPD X12, X11
	MOVAPD X11, 16(R10)

	ADDQ $32, R8
	ADDQ $32, R10
	SUBQ $4, SI
	JG loop

end:	
	RET

// func innrPrd_sse3(a,b,c []float64)
TEXT ·innrPrd_sse2(SB), NOSPLIT, $0
	MOVQ a_base+0(FP), R8
	MOVQ b_base+24(FP), R9
	MOVQ c_base+48(FP), R10
	MOVQ $16, SI  // Hard-coded for 4x4 matrix

	MOVAPD (R9), X0
	MOVAPD 16(R9), X4
	MOVAPD 32(R9), X8
	MOVAPD 48(R9), X9

	MOVAPD X0, X2
	UNPCKLPD X8, X0
	UNPCKHPD X8, X2
	
	MOVAPD X4, X6
	UNPCKLPD X9, X4
	UNPCKHPD X9, X6

	MOVAPD 64(R9), X1
	MOVAPD 80(R9), X5
	MOVAPD 96(R9), X8
	MOVAPD 112(R9), X9
	
	MOVAPD X1, X3
	UNPCKLPD X8, X1
	UNPCKHPD X8, X3

	MOVAPD X5, X7
	UNPCKLPD X9, X5
	UNPCKHPD X9, X7
	// Column data is now organized in xmm0-7
	
loop:	// Load single Row data
	MOVAPD (R8), X8
	MOVAPD X8, X10
	MOVAPD 16(R8), X9
	MOVAPD X9, X11

	MULPD X0, X10
	MULPD X1, X11
	ADDPD X10, X11

	MOVAPD X8, X10
	MOVAPD X9, X12

	MULPD X2, X10
	MULPD X3, X12
	ADDPD X10, X12

	MOVAPD X11, X10
	UNPCKLPD X12, X11
	UNPCKHPD X12, X10
	ADDPD X10, X11 
	
	MOVAPD X11, (R10)

	MOVAPD X8, X10
	MOVAPD X9, X11

	MULPD X4, X10
	MULPD X5, X11
	ADDPD X10, X11

	MOVAPD X8, X10
	MOVAPD X9, X12

	MULPD X6, X10
	MULPD X7, X12
	ADDPD X10, X12

	MOVAPD X11, X10
	UNPCKLPD X12, X11
	UNPCKHPD X12, X10
	ADDPD X10, X11 
	
	MOVAPD X11, 16(R10)

	ADDQ $32, R8
	ADDQ $32, R10
	SUBQ $4, SI
	JG loop
end:	
	RET

// func initasm()(a,a2 bool)
// pulled from runtime/asm_amd64.s
TEXT ·testBroadcast(SB), NOSPLIT, $0
	MOVQ a_base+0(FP), R8
	MOVQ b_base+24(FP), DI
	MOVQ c_base+48(FP), R10

	MOVQ $0x60402000, R15
	MOVQ R15, X1
	//VPMOVZXBQ Y14, X1  //Indexes 0,32,64,96
	BYTE $0xC4; BYTE $0x62; BYTE $0x7D; BYTE $0x32; BYTE $0xF1

	//VCMPEQPD Y1, Y1, Y1
	BYTE $0xC5    ; BYTE $0xF5; BYTE $0xC2; BYTE $0xC9; BYTE $0x00
	//VGATHERQPD [Y14+DI], Y0, Y1
	BYTE $0xC4    ; BYTE $0xA2; BYTE $0xF5; BYTE $0x93; BYTE $0x04; BYTE $0x37
	
	//VCMPEQPD Y2, Y2, Y2
	BYTE $0xC5; BYTE $0xED; BYTE $0xC2; BYTE $0xD2; BYTE $0x00
	//VGATHERQPD [Y14+8+DI], Y1, Y2
	BYTE $0xC4; BYTE $0xA2; BYTE $0xED; BYTE $0x93; BYTE $0x4C; BYTE $0x37; BYTE $0x08

	//VCMPEQPD Y3, Y3 Y3
	BYTE $0xC5; BYTE $0xE5; BYTE $0xC2; BYTE $0xDB; BYTE $0x00
	//VGATHERQPD [Y14+16+DI], Y2, Y3
	BYTE $0xC4; BYTE $0xA2; BYTE $0xE5; BYTE $0x93; BYTE $0x54; BYTE $0x37; BYTE $0x10

	//VCMPEQPD Y4, Y4 Y4
	BYTE $0xC5; BYTE $0xDD; BYTE $0xC2; BYTE $0xE4; BYTE $0x00
	//VGATHERQPD [Y14+32+DI], Y3, Y4
	BYTE $0xC4; BYTE $0xA2; BYTE $0xDD; BYTE $0x93; BYTE $0x5C; BYTE $0x37; BYTE $0x18

	//VMULPD (R8), Y0, Y5
	BYTE $0xC4; BYTE $0xC1; BYTE $0x7D; BYTE $0x59; BYTE $0x28;
	//VMULPD (R8), Y1, Y6
	BYTE $0xC4; BYTE $0xC1; BYTE $0x75; BYTE $0x59; BYTE $0x30;
	//VMULPD (R8), Y2, Y7
	BYTE $0xC4; BYTE $0xC1; BYTE $0x6D; BYTE $0x59; BYTE $0x38;
	//VMULPD (R8), Y3, Y8
	BYTE $0xC4; BYTE $0x41; BYTE $0x65; BYTE $0x59; BYTE $0x00;

	//VMULPD 32(R8), Y0, Y10
	BYTE $0xC4; BYTE $0x41; BYTE $0x7D; BYTE $0x59; BYTE $0x50; BYTE $0x20;
	//VMULPD 32(R8), Y1, Y11
	BYTE $0xC4; BYTE $0x41; BYTE $0x75; BYTE $0x59; BYTE $0x58; BYTE $0x20;
	//VMULPD 32(R8), Y2, Y12
	BYTE $0xC4; BYTE $0x41; BYTE $0x6D; BYTE $0x59; BYTE $0x60; BYTE $0x20;
	//VMULPD 32(R8), Y3, Y13
	BYTE $0xC4; BYTE $0x41; BYTE $0x65; BYTE $0x59; BYTE $0x68; BYTE $0x20; 

	//VHADDPD Y5, Y6, Y5
	BYTE $0xC5; BYTE $0xD5; BYTE $0x7C; BYTE $0xEE	
	//VHADDPD Y7, Y8, Y6
	BYTE $0xC4; BYTE $0xC1; BYTE $0x45; BYTE $0x7C; BYTE $0xF0
	
	//VHADDPD Y10, Y11, Y10
	BYTE $0xC4; BYTE $0x41; BYTE $0x2D; BYTE $0x7C; BYTE $0xD3
	//VHADDPD Y12, Y13, Y11
	BYTE $0xC4; BYTE $0x41; BYTE $0x1D; BYTE $0x7C; BYTE $0xDD
	
	//VPERM2F128 Y5, Y6, Y7, $0x31	
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x06; BYTE $0xFE; BYTE $0x31
	//VPERM2F128 Y5, Y6, Y6, $0x20
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x06; BYTE $0xF6; BYTE $0x20

	//VADDPD Y6, Y7, Y4
	BYTE $0xC5; BYTE $0xCD; BYTE $0x58; BYTE $0xE7
	//VMOVAPD Y4, (R10)
	//BYTE $0xC4; BYTE $0xC1; BYTE $0x7D; BYTE $0x29; BYTE $0x22;
	VMOVDQA Y4, (R10)

	//VPERM2F128 Y10, Y11, Y12, $0x20
	BYTE $0xC4; BYTE $0x43; BYTE $0x2D; BYTE $0x06; BYTE $0xE3; BYTE $0x20
	//VERPM2F128 Y10, Y11, Y13, $0X31
	BYTE $0xC4; BYTE $0x43; BYTE $0x2D; BYTE $0x06; BYTE $0xEB; BYTE $0x31
	
	//VADDPD Y12, Y13, Y9
	BYTE $0xC4; BYTE $0x41; BYTE $0x1D; BYTE $0x58; BYTE $0xCD
	//VMOVAPD Y9, 32(R10)
	//BYTE $0xC4; BYTE $0x41; BYTE $0x7D; BYTE $0x29; BYTE $0x4A; BYTE $0x20;
	VMOVDQA Y9, 32(R10)
	
	//VMULPD 64(R8), Y0, Y5
	BYTE $0xC4; BYTE $0xC1; BYTE $0x7D; BYTE $0x59; BYTE $0x68; BYTE $0x40;
	//VMULPD 64(R8), Y1, Y6
	BYTE $0xC4; BYTE $0xC1; BYTE $0x75; BYTE $0x59; BYTE $0x70; BYTE $0x40;
	//VMULPD 64(R8), Y2, Y7
	BYTE $0xC4; BYTE $0xC1; BYTE $0x6D; BYTE $0x59; BYTE $0x78; BYTE $0x40;
	//VMULPD 64(R8), Y3, Y8
	BYTE $0xC4; BYTE $0x41; BYTE $0x65; BYTE $0x59; BYTE $0x40; BYTE $0x40;

	//VMULPD 96(R8), Y0, Y10
	BYTE $0xC4; BYTE $0x41; BYTE $0x7D; BYTE $0x59; BYTE $0x50; BYTE $0x60;
	//VMULPD 96(R8), Y1, Y11
	BYTE $0xC4; BYTE $0x41; BYTE $0x75; BYTE $0x59; BYTE $0x58; BYTE $0x60;
	//VMULPD 96(R8), Y2, Y12
	BYTE $0xC4; BYTE $0x41; BYTE $0x6D; BYTE $0x59; BYTE $0x60; BYTE $0x60;
	//VMULPD 96(R8), Y3, Y13
	BYTE $0xC4; BYTE $0x41; BYTE $0x65; BYTE $0x59; BYTE $0x68; BYTE $0x60; 

	//VHADDPD Y5, Y6, Y5
	BYTE $0xC5; BYTE $0xD5; BYTE $0x7C; BYTE $0xEE	
	//VHADDPD Y7, Y8, Y6
	BYTE $0xC4; BYTE $0xC1; BYTE $0x45; BYTE $0x7C; BYTE $0xF0

	//VHADDPD Y10, Y11, Y10
	BYTE $0xC4; BYTE $0x41; BYTE $0x2D; BYTE $0x7C; BYTE $0xD3
	//VHADDPD Y12, Y13, Y11
	BYTE $0xC4; BYTE $0x41; BYTE $0x1D; BYTE $0x7C; BYTE $0xDD

	//VPERM2F128 Y5, Y6, Y7, $0x31	
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x06; BYTE $0xFE; BYTE $0x31
	//VPERM2F128 Y5, Y6, Y6, $0x20
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x06; BYTE $0xF6; BYTE $0x20

	//VADDPD Y6, Y7, Y4
	BYTE $0xC5; BYTE $0xCD; BYTE $0x58; BYTE $0xE7
	//VMOVAPD Y4, 64(R10)
	//BYTE $0xC4; BYTE $0xC1; BYTE $0x7D; BYTE $0x29; BYTE $0x62; BYTE $0x40;
	VMOVDQA Y4, 64(R10)


	//VPERM2F128 Y10, Y11, Y12, $0x20
	BYTE $0xC4; BYTE $0x43; BYTE $0x2D; BYTE $0x06; BYTE $0xE3; BYTE $0x20
	//VERPM2F128 Y10, Y11, Y13, $0X31
	BYTE $0xC4; BYTE $0x43; BYTE $0x2D; BYTE $0x06; BYTE $0xEB; BYTE $0x31
	
	//VADDPD Y12, Y13, Y9
	BYTE $0xC4; BYTE $0x41; BYTE $0x1D; BYTE $0x58; BYTE $0xCD
	//VMOVAPD Y9, 96(R10)
	//BYTE $0xC4; BYTE $0x41; BYTE $0x7D; BYTE $0x29; BYTE $0x4A; BYTE $0x60;
	VMOVDQA Y9, 96(R10)
	
	//Cleanup
	VZEROUPPER

	RET

TEXT ·testBroadcast2(SB), NOSPLIT, $0
	MOVQ b_base+24(FP), DI
	PREFETCHT0 (DI)
	MOVQ a_base+0(FP), R8
	MOVQ c_base+48(FP), R10

	MOVQ $0x60402000, R15
	MOVQ R15, X14
	//VPMOVZXBQ Y14, X14  //Indexes 0,32,64,96
	BYTE $0xC4; BYTE $0x42; BYTE $0x7D; BYTE $0x32; BYTE $0xF6
	//MOVQ R15, X1
	//BYTE $0xC4; BYTE $0x62; BYTE $0x7D; BYTE $0x32; BYTE $0xF1

	//VCMPEQPD Y1, Y1, Y1
	BYTE $0xC5    ; BYTE $0xF5; BYTE $0xC2; BYTE $0xC9; BYTE $0x00
	//VGATHERQPD [Y14+DI], Y0, Y1
	BYTE $0xC4    ; BYTE $0xA2; BYTE $0xF5; BYTE $0x93; BYTE $0x04; BYTE $0x37
	
	//VCMPEQPD Y2, Y2, Y2
	BYTE $0xC5; BYTE $0xED; BYTE $0xC2; BYTE $0xD2; BYTE $0x00
	//VGATHERQPD [Y14+8+DI], Y1, Y2
	BYTE $0xC4; BYTE $0xA2; BYTE $0xED; BYTE $0x93; BYTE $0x4C; BYTE $0x37; BYTE $0x08

	//VCMPEQPD Y3, Y3 Y3
	BYTE $0xC5; BYTE $0xE5; BYTE $0xC2; BYTE $0xDB; BYTE $0x00
	//VGATHERQPD [Y14+16+DI], Y2, Y3
	BYTE $0xC4; BYTE $0xA2; BYTE $0xE5; BYTE $0x93; BYTE $0x54; BYTE $0x37; BYTE $0x10

	//VCMPEQPD Y4, Y4 Y4
	BYTE $0xC5; BYTE $0xDD; BYTE $0xC2; BYTE $0xE4; BYTE $0x00
	//VGATHERQPD [Y14+32+DI], Y3, Y4
	BYTE $0xC4; BYTE $0xA2; BYTE $0xDD; BYTE $0x93; BYTE $0x5C; BYTE $0x37; BYTE $0x18

	//VMOVAPD (R8), Y4
	//BYTE $0xC4; BYTE $0xC1; BYTE $0x7D; BYTE $0x28; BYTE $0x20;
	VMOVDQA (R8), Y4

	//VMULPD Y4, Y0, Y5
	BYTE $0xC5; BYTE $0xFD; BYTE $0x59; BYTE $0xEC;
	//VMULPD Y4, Y1, Y6
	BYTE $0xC5; BYTE $0xF5; BYTE $0x59; BYTE $0xF4;

	PREFETCHT0 (R10)

	//VMULPD Y4, Y2, Y7
	BYTE $0xC5; BYTE $0xED; BYTE $0x59; BYTE $0xFC;
	//VMULPD Y4, Y3, Y8
	BYTE $0xC5; BYTE $0x65; BYTE $0x59; BYTE $0xC4;

	//VMOVAPD 32(R8), Y9
	//BYTE $0xC4; BYTE $0x41; BYTE $0x7D; BYTE $0x28; BYTE $0x48; BYTE $0x20;
	VMOVDQA 32(R8), Y9
	
	//VMULPD Y9, Y0, Y10
	BYTE $0xC4; BYTE $0x41; BYTE $0x7D; BYTE $0x59; BYTE $0xD1;
	//VMULPD Y9, Y1, Y11
	BYTE $0xC4; BYTE $0x41; BYTE $0x75; BYTE $0x59; BYTE $0xD9;

	//VMULPD Y9, Y2, Y12
	BYTE $0xC4; BYTE $0x41; BYTE $0x6D; BYTE $0x59; BYTE $0xE1;
	//VMULPD Y9, Y3, Y13
	BYTE $0xC4; BYTE $0x41; BYTE $0x65; BYTE $0x59; BYTE $0xE9; 

	//VMOVAPD 64(R8), Y14
	//BYTE $0xC4; BYTE $0x41; BYTE $0x7D; BYTE $0x28; BYTE $0x70; BYTE $0x40;
	VMOVDQA 64(R8), Y14
	
	//VHADDPD Y5, Y6, Y5
	BYTE $0xC5; BYTE $0xD5; BYTE $0x7C; BYTE $0xEE	
	//VHADDPD Y7, Y8, Y6
	BYTE $0xC4; BYTE $0xC1; BYTE $0x45; BYTE $0x7C; BYTE $0xF0
	
	//VHADDPD Y10, Y11, Y10
	BYTE $0xC4; BYTE $0x41; BYTE $0x2D; BYTE $0x7C; BYTE $0xD3
	//VHADDPD Y12, Y13, Y11
	BYTE $0xC4; BYTE $0x41; BYTE $0x1D; BYTE $0x7C; BYTE $0xDD

	//VMOVAPD 96(R8), Y15
	//BYTE $0xC4; BYTE $0x41; BYTE $0x7D; BYTE $0x28; BYTE $0x78; BYTE $0x60;
	VMOVDQA 96(R8), Y15
	
	//VPERM2F128 Y5, Y6, Y7, $0x31	
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x06; BYTE $0xFE; BYTE $0x31
	//VPERM2F128 Y5, Y6, Y6, $0x20
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x06; BYTE $0xF6; BYTE $0x20

	//VADDPD Y6, Y7, Y4
	BYTE $0xC5; BYTE $0xCD; BYTE $0x58; BYTE $0xE7
	//VMOVAPD Y4, (R10)
	//BYTE $0xC4; BYTE $0xC1; BYTE $0x7D; BYTE $0x29; BYTE $0x22;
	VMOVDQA Y4, (R10)

	//VPERM2F128 Y10, Y11, Y12, $0x20
	BYTE $0xC4; BYTE $0x43; BYTE $0x2D; BYTE $0x06; BYTE $0xE3; BYTE $0x20
	//VERPM2F128 Y10, Y11, Y13, $0X31
	BYTE $0xC4; BYTE $0x43; BYTE $0x2D; BYTE $0x06; BYTE $0xEB; BYTE $0x31
	
	//VADDPD Y12, Y13, Y9
	BYTE $0xC4; BYTE $0x41; BYTE $0x1D; BYTE $0x58; BYTE $0xCD	
	//VMOVAPD Y9, 32(R10)
	//BYTE $0xC4; BYTE $0x41; BYTE $0x7D; BYTE $0x29; BYTE $0x4A; BYTE $0x20;
	VMOVDQA Y9, 32(R10)
	
	//VMULPD Y14, Y0, Y5
	BYTE $0xC4; BYTE $0xC1; BYTE $0x7D; BYTE $0x59; BYTE $0xEE;
	//VMULPD Y14, Y1, Y6
	BYTE $0xC4; BYTE $0xC1; BYTE $0x75; BYTE $0x59; BYTE $0xF6;
	//VMULPD Y14, Y2, Y7
	BYTE $0xC4; BYTE $0xC1; BYTE $0x6D; BYTE $0x59; BYTE $0xFE;
	//VMULPD Y14, Y3, Y8
	BYTE $0xC4; BYTE $0x41; BYTE $0x65; BYTE $0x59; BYTE $0xC6; 

	PREFETCHT0 64(R10)
	
	//VMULPD Y15, Y0, Y10
	BYTE $0xC4; BYTE $0x41; BYTE $0x7D; BYTE $0x59; BYTE $0xD7;
	//VMULPD Y15, Y1, Y11
	BYTE $0xC4; BYTE $0x41; BYTE $0x75; BYTE $0x59; BYTE $0xDF;
	//VMULPD Y15, Y2, Y12
	BYTE $0xC4; BYTE $0x41; BYTE $0x6D; BYTE $0x59; BYTE $0xE7;
	//VMULPD Y15, Y3, Y13
	BYTE $0xC4; BYTE $0x41; BYTE $0x65; BYTE $0x59; BYTE $0xEF;

	//VHADDPD Y5, Y6, Y5
	BYTE $0xC5; BYTE $0xD5; BYTE $0x7C; BYTE $0xEE	
	//VHADDPD Y7, Y8, Y6
	BYTE $0xC4; BYTE $0xC1; BYTE $0x45; BYTE $0x7C; BYTE $0xF0

	//VHADDPD Y10, Y11, Y10
	BYTE $0xC4; BYTE $0x41; BYTE $0x2D; BYTE $0x7C; BYTE $0xD3
	//VHADDPD Y12, Y13, Y11
	BYTE $0xC4; BYTE $0x41; BYTE $0x1D; BYTE $0x7C; BYTE $0xDD

	//VPERM2F128 Y5, Y6, Y7, $0x31	
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x06; BYTE $0xFE; BYTE $0x31
	//VPERM2F128 Y5, Y6, Y6, $0x20
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x06; BYTE $0xF6; BYTE $0x20

	//VADDPD Y6, Y7, Y4
	BYTE $0xC5; BYTE $0xCD; BYTE $0x58; BYTE $0xE7
	//VMOVAPD Y4, 64(R10)
	//BYTE $0xC4; BYTE $0xC1; BYTE $0x7D; BYTE $0x29; BYTE $0x62; BYTE $0x40;
	VMOVDQA Y4, 64(R10)

	//VPERM2F128 Y10, Y11, Y12, $0x20
	BYTE $0xC4; BYTE $0x43; BYTE $0x2D; BYTE $0x06; BYTE $0xE3; BYTE $0x20
	//VERPM2F128 Y10, Y11, Y13, $0X31
	BYTE $0xC4; BYTE $0x43; BYTE $0x2D; BYTE $0x06; BYTE $0xEB; BYTE $0x31
	
	//VADDPD Y12, Y13, Y9
	BYTE $0xC4; BYTE $0x41; BYTE $0x1D; BYTE $0x58; BYTE $0xCD
	//VMOVAPD Y9, 96(R10)
	//BYTE $0xC4; BYTE $0x41; BYTE $0x7D; BYTE $0x29; BYTE $0x4A; BYTE $0x60;
	VMOVDQA Y9, 96(R10)
	
	//Cleanup
	VZEROUPPER

	RET
